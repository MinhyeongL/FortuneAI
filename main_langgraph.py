"""
FortuneAI LangGraph ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
Supervisor íŒ¨í„´ ê¸°ë°˜ ì‚¬ì£¼ ê³„ì‚°ê¸°
"""

import os
import sys
import time
import uuid
import logging
from datetime import datetime

# httpx ë¡œê¹… ë¹„í™œì„±í™” (OpenAI API í˜¸ì¶œ ë¡œê·¸ ìˆ¨ê¹€)
logging.getLogger("httpx").setLevel(logging.WARNING)

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from langchain_core.messages import HumanMessage
from langgraph_system.graph import create_workflow

def print_banner():
    """ì‹œìŠ¤í…œ ë°°ë„ˆ ì¶œë ¥"""
    print("=" * 60)
    print("ğŸ”® FortuneAI - LangGraph ì‚¬ì£¼ ì‹œìŠ¤í…œ ğŸ”®")
    print("=" * 60)
    print("âœ¨ Supervisor íŒ¨í„´ ê¸°ë°˜ ê³ ì„±ëŠ¥ ì‚¬ì£¼ ê³„ì‚°ê¸°")
    print("ğŸ¯ 98ì  ì „ë¬¸ê°€ ê²€ì¦ ì™„ë£Œ")
    print("ğŸš€ LangGraph ë©€í‹° ì›Œì»¤ ì‹œìŠ¤í…œ")
    print("-" * 60)
    print("ğŸ“ ì‚¬ìš©ë²•:")
    print("  â€¢ ì‚¬ì£¼ ê³„ì‚°: '1995ë…„ 8ì›” 26ì¼ ì˜¤ì „ 10ì‹œ 15ë¶„ ë‚¨ì ì‚¬ì£¼'")
    print("  â€¢ ìš´ì„¸ ìƒë‹´: '1995ë…„ 8ì›” 26ì¼ìƒ 2024ë…„ ì—°ì• ìš´'")
    print("  â€¢ ì¼ë°˜ ê²€ìƒ‰: 'ì‚¬ì£¼ì—ì„œ ì‹­ì‹ ì´ë€?'")
    print("  â€¢ ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit'")
    print("  â€¢ ë””ë²„ê·¸: 'debug:ì§ˆë¬¸' (ìƒì„¸ ì‹¤í–‰ ì •ë³´)")
    print("  â€¢ ìƒì„¸ëª¨ë“œ: 'verbose:ì§ˆë¬¸' (ë…¸ë“œë³„ ìƒì„¸ ë¡œê¹…)")
    print("  â€¢ ê³ ê¸‰ìŠ¤íŠ¸ë¦¬ë°: 'stream:ì§ˆë¬¸' (ì£¼ìš” ë…¸ë“œë§Œ ì‹¤ì‹œê°„ ì¶œë ¥)")
    print("=" * 60)

def print_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
    print("\nğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"  â€¢ ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  â€¢ Python ë²„ì „: {sys.version.split()[0]}")
    print(f"  â€¢ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"  â€¢ ì›Œì»¤ ë…¸ë“œ: Supervisor, ì‚¬ì£¼ê³„ì‚°, RAGê²€ìƒ‰, ì›¹ê²€ìƒ‰, ì‘ë‹µìƒì„±")
    print()

def format_response(response: str) -> str:
    """ì‘ë‹µ í¬ë§·íŒ…"""
    if not response:
        return "âŒ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì‘ë‹µ ì•ì— êµ¬ë¶„ì„  ì¶”ê°€
    formatted = "\n" + "ğŸ¯ " + "=" * 55 + "\n"
    formatted += "ğŸ“‹ **FortuneAI ë¶„ì„ ê²°ê³¼**\n"
    formatted += "=" * 58 + "\n\n"
    formatted += response
    formatted += "\n\n" + "=" * 58
    
    return formatted

def handle_debug_query(query: str, app, conversation_history: list) -> str:
    """ë””ë²„ê·¸ ì¿¼ë¦¬ ì²˜ë¦¬"""
    if not query.startswith("debug:"):
        return None
    
    actual_query = query[6:].strip()
    if not actual_query:
        return "âŒ ë””ë²„ê·¸í•  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: debug:1995ë…„ 8ì›” 26ì¼ ì‚¬ì£¼"
    
    print(f"\nğŸ” ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘: '{actual_query}'")
    print("-" * 50)
    
    start_time = time.time()
    response = run_query_with_app(actual_query, app, conversation_history)
    execution_time = time.time() - start_time
    
    debug_info = f"""
ğŸ” **ë””ë²„ê·¸ ì •ë³´**
â€¢ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ
â€¢ ì§ˆë¬¸: {actual_query}

ğŸ“‹ **ì‘ë‹µ**
{response}
"""
    return debug_info

def handle_verbose_query(query: str, app, conversation_history: list) -> str:
    """ìƒì„¸ ëª¨ë“œ ì¿¼ë¦¬ ì²˜ë¦¬"""
    if not query.startswith("verbose:"):
        return None
    
    actual_query = query[8:].strip()
    if not actual_query:
        return "âŒ ìƒì„¸ ëª¨ë“œë¡œ ì‹¤í–‰í•  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: verbose:1995ë…„ 8ì›” 26ì¼ ì‚¬ì£¼"
    
    print(f"\nğŸ” ìƒì„¸ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘: '{actual_query}'")
    print("=" * 60)
    
    start_time = time.time()
    response = run_query_with_app(actual_query, app, conversation_history)
    execution_time = time.time() - start_time
    
    print(f"\nâ±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
    return response

def handle_stream_query(query: str, app, conversation_history: list) -> str:
    """ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬ - íŠ¹ì • ë…¸ë“œë§Œ í•„í„°ë§"""
    if not query.startswith("stream:"):
        return None
    
    actual_query = query[7:].strip()
    if not actual_query:
        return "âŒ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë°í•  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: stream:1995ë…„ 8ì›” 26ì¼ ì‚¬ì£¼"
    
    print(f"\nğŸŒŠ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘: '{actual_query}'")
    print("ğŸ“Œ ì£¼ìš” ë…¸ë“œ(rag_agent, response_generator)ë§Œ í‘œì‹œë©ë‹ˆë‹¤")
    print("=" * 60)
    
    start_time = time.time()
    response = run_query_with_streaming(actual_query, app, conversation_history)
    execution_time = time.time() - start_time
    
    print(f"\nâ±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
    return response

def run_query_with_app(query: str, app, conversation_history: list) -> str:
    """LangGraph ì‹œìŠ¤í…œìœ¼ë¡œ ì¿¼ë¦¬ ì‹¤í–‰ - stream_graph ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥"""
    print(f"ğŸ” ì¿¼ë¦¬ ì‹¤í–‰: {query}")
    
    # ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    conversation_history.append(HumanMessage(content=query))
    
    # í˜„ì¬ ìƒíƒœ ì„¤ì • (ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
    current_state = {
        "messages": conversation_history.copy(),
        "next": None,
        "final_response": None,
        "sender": None
    }
    
    try:
        print("ğŸš€ ì›Œí¬í”Œë¡œ ì‹¤í–‰ ì¤‘...")
        
        # stream_graph ë°©ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        final_response = ""
        prev_node = ""
        
        for chunk_msg, metadata in app.stream(current_state, stream_mode="messages"):
            curr_node = metadata["langgraph_node"]
            
            # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
            if curr_node != prev_node:
                print("\n" + "=" * 50)
                print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
                print("- " * 25)
            
            # í† í°ë³„ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥
            if chunk_msg.content:
                print(chunk_msg.content, end="", flush=True)
            
            prev_node = curr_node
        
        print("\n" + "=" * 50)
        print("âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ!")
        
        # ìµœì¢… ì‘ë‹µì„ ì–»ê¸° ìœ„í•´ í•œ ë²ˆ ë” ì‹¤í–‰ (final_response íšë“ìš©)
        result = app.invoke(current_state)
        final_response = result.get("final_response")
        
        if final_response:
            # AI ì‘ë‹µë„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            from langchain_core.messages import AIMessage
            conversation_history.append(AIMessage(content=final_response))
            return final_response
        else:
            print("âŒ ìµœì¢… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
            return "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def run_query_with_streaming(query: str, app, conversation_history: list) -> str:
    """í–¥ìƒëœ ìŠ¤íŠ¸ë¦¬ë° - íŠ¹ì • ë…¸ë“œë§Œ í•„í„°ë§í•˜ì—¬ ì¶œë ¥"""
    print(f"ğŸ” ì¿¼ë¦¬ ì‹¤í–‰ (ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë°): {query}")
    
    # ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    conversation_history.append(HumanMessage(content=query))
    
    current_state = {
        "messages": conversation_history.copy(),
        "next": None,
        "final_response": None,
        "sender": None
    }
    
    try:
        print("ğŸš€ ì›Œí¬í”Œë¡œ ì‹¤í–‰ ì¤‘...")
        
        # stream_graph ë°©ì‹ + ë…¸ë“œ í•„í„°ë§
        final_response = ""
        prev_node = ""
        target_nodes = ["rag_agent", "response_generator"]  # ì›í•˜ëŠ” ë…¸ë“œë§Œ ì¶œë ¥
        
        for chunk_msg, metadata in app.stream(current_state, stream_mode="messages"):
            curr_node = metadata["langgraph_node"]
            
            # íŠ¹ì • ë…¸ë“œë“¤ë§Œ ì¶œë ¥
            if curr_node in target_nodes:
                # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
                if curr_node != prev_node:
                    print("\n" + "=" * 50)
                    print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
                    print("- " * 25)
                
                # í† í°ë³„ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥
                if chunk_msg.content:
                    print(chunk_msg.content, end="", flush=True)
                
                prev_node = curr_node
        
        print("\n" + "=" * 50)
        print("âœ… ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ!")
        
        # ìµœì¢… ì‘ë‹µ íšë“
        result = app.invoke(current_state)
        final_response = result.get("final_response")
        
        if final_response:
            from langchain_core.messages import AIMessage
            conversation_history.append(AIMessage(content=final_response))
            return final_response
        else:
            print("âŒ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
            return run_query_with_app(query, app, conversation_history[:-1])
            
    except Exception as e:
        print(f"âŒ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("ğŸ”„ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
        return run_query_with_app(query, app, conversation_history[:-1])

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_banner()
    print_system_info()
    
    # âœ¨ ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ NodeManager ë¯¸ë¦¬ ì´ˆê¸°í™”
    print("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    from langgraph_system.nodes import get_node_manager
    get_node_manager()  # ì‹±ê¸€í†¤ ì´ˆê¸°í™” (6-10ì´ˆ ì†Œìš”)
    print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    # âœ¨ ì›Œí¬í”Œë¡œë„ ë¯¸ë¦¬ ìƒì„±
    print("âš™ï¸ ì›Œí¬í”Œë¡œ ìƒì„± ì¤‘...")
    app = create_workflow()
    print("âœ… ì›Œí¬í”Œë¡œ ì¤€ë¹„ ì™„ë£Œ!")
    
    session_id = f"session_{int(time.time())}"
    query_count = 0
    conversation_history = []  # ğŸ”¥ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    
    print("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì¢…ë£Œ: quit/exit):")
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            user_input = input("\nğŸ¤” ì§ˆë¬¸: ").strip()
            
            # ì¢…ë£Œ ëª…ë ¹ ì²˜ë¦¬
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\nğŸ‘‹ FortuneAIë¥¼ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!")
                print("ğŸŒŸ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”! ğŸŒŸ")
                break
            
            # ìƒˆ ì„¸ì…˜ ì‹œì‘ ëª…ë ¹ ì²˜ë¦¬
            if user_input.lower() == 'new':
                session_id = str(uuid.uuid4())
                query_count = 0
                conversation_history = []  # ğŸ”¥ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
                print(f"\nğŸ”„ ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (ì„¸ì…˜ ID: {session_id[:8]}...)")
                
                # í™˜ì˜ ë©”ì‹œì§€ ìƒì„±
                welcome_response = run_query_with_app("ì•ˆë…•í•˜ì„¸ìš”! FortuneAIì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?", app, conversation_history)
                print(f"ğŸ”® FortuneAI: {welcome_response}")
                print("-" * 60)
                continue
            
            # ë„ì›€ë§ ëª…ë ¹ ì²˜ë¦¬
            if user_input.lower() in ['help', 'h', 'ë„ì›€ë§', '?']:
                print("""
ğŸ“š **FortuneAI ì‚¬ìš© ê°€ì´ë“œ**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”® **ì‚¬ì£¼ ê³„ì‚°**: '1995ë…„ 8ì›” 26ì¼ ì˜¤ì „ 10ì‹œ 15ë¶„ ë‚¨ì ì‚¬ì£¼'
ğŸ“– **ì‚¬ì£¼ í•´ì„**: 'ì‚¬ì£¼ì—ì„œ ì‹­ì‹ ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?'
ğŸŒ **ì¼ë°˜ ì§ˆë¬¸**: '2024ë…„ ê°‘ì§„ë…„ì˜ íŠ¹ì§•ì€?'

ğŸ› ï¸  **íŠ¹ìˆ˜ ëª…ë ¹ì–´**:
  â€¢ new, clear      : ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘
  â€¢ help, ?         : ë„ì›€ë§ ë³´ê¸°
  â€¢ quit, exit      : í”„ë¡œê·¸ë¨ ì¢…ë£Œ
  â€¢ debug:ì§ˆë¬¸      : ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
  â€¢ verbose:ì§ˆë¬¸    : ìƒì„¸ ëª¨ë“œë¡œ ì‹¤í–‰
  â€¢ stream:ì§ˆë¬¸     : ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° (ì£¼ìš” ë…¸ë“œë§Œ)

ğŸŒŠ **ìŠ¤íŠ¸ë¦¬ë° ì„¤ëª…**:
  â€¢ ê¸°ë³¸ ì‹¤í–‰: ëª¨ë“  ë…¸ë“œì˜ í† í°ì„ ì‹¤ì‹œê°„ í‘œì‹œ (stream_graph ë°©ì‹)
  â€¢ stream: ëª…ë ¹: ì£¼ìš” ë…¸ë“œ(rag_agent, response_generator)ë§Œ í‘œì‹œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                """)
                continue
            
            # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
            if not user_input:
                print("â“ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            query_count += 1
            print(f"\nâ³ ë¶„ì„ ì¤‘... (ì§ˆë¬¸ #{query_count})")
            
            # ë””ë²„ê·¸ ëª¨ë“œ ì²˜ë¦¬
            if user_input.startswith("debug:"):
                response = handle_debug_query(user_input, app, conversation_history)
                print(response)
                continue
            
            # ìƒì„¸ ëª¨ë“œ ì²˜ë¦¬
            if user_input.startswith("verbose:"):
                response = handle_verbose_query(user_input, app, conversation_history)
                print(f"\nğŸ“ **ìµœì¢… ì‘ë‹µ**\n{format_response(response)}")
                continue
            
            # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬
            if user_input.startswith("stream:"):
                response = handle_stream_query(user_input, app, conversation_history)
                print(f"\nğŸ“ **ìµœì¢… ì‘ë‹µ**\n{format_response(response)}")
                continue
            
            # ì¼ë°˜ ì¿¼ë¦¬ ì‹¤í–‰ - ë¯¸ë¦¬ ìƒì„±ëœ ì›Œí¬í”Œë¡œ ì‚¬ìš©
            start_time = time.time()
            response = run_query_with_app(user_input, app, conversation_history)  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ë‹¬
            execution_time = time.time() - start_time
            
            # ì‘ë‹µ ì¶œë ¥
            formatted_response = format_response(response)
            print(formatted_response)
            
            # ì‹¤í–‰ ì‹œê°„ í‘œì‹œ
            print(f"\nâ±ï¸  ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            print("ğŸ‘‹ FortuneAIë¥¼ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!")
            break
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            print("ğŸ”§ ì‹œìŠ¤í…œì„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
            continue

if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        if sys.argv[1] == "debug":
            if len(sys.argv) > 2:
                query = " ".join(sys.argv[2:])
                # ëª…ë ¹í–‰ì—ì„œëŠ” ì›Œí¬í”Œë¡œë¥¼ ìƒˆë¡œ ìƒì„±
                print("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
                from langgraph_system.nodes import get_node_manager
                get_node_manager()
                print("âš™ï¸ ì›Œí¬í”Œë¡œ ìƒì„± ì¤‘...")
                app = create_workflow()
                conversation_history = []
                result = handle_debug_query(f"debug:{query}", app, conversation_history)
                print(result)
            else:
                print("âŒ ë””ë²„ê·¸í•  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                print("ì‚¬ìš©ë²•: python main_langgraph.py debug '1995ë…„ 8ì›” 26ì¼ ì‚¬ì£¼'")
        else:
            # ì§ì ‘ ì¿¼ë¦¬ ì‹¤í–‰
            query = " ".join(sys.argv[1:])
            print("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            from langgraph_system.nodes import get_node_manager
            get_node_manager()
            print("âš™ï¸ ì›Œí¬í”Œë¡œ ìƒì„± ì¤‘...")
            app = create_workflow()
            conversation_history = []
            response = run_query_with_app(query, app, conversation_history)
            print(format_response(response))
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        main() 