"""
FortuneAI ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
UI, ì¿¼ë¦¬ ì²˜ë¦¬, ë””ìŠ¤í”Œë ˆì´ ê´€ë ¨ ëª¨ë“  ê¸°ëŠ¥ í†µí•©
"""

import os
import sys
import time
from datetime import datetime
from langchain_core.messages import HumanMessage, AIMessage


# ================================
# UI / ë””ìŠ¤í”Œë ˆì´ ê´€ë ¨ í•¨ìˆ˜ë“¤
# ================================

def print_banner():
    """ì‹œìŠ¤í…œ ë°°ë„ˆ ì¶œë ¥"""
    print("=" * 60)
    print("ğŸ”® FortuneAI - LangGraph ì‚¬ì£¼ ì‹œìŠ¤í…œ ğŸ”®")
    print("=" * 60)
    print("âœ¨ Supervisor íŒ¨í„´ ê¸°ë°˜ ê³ ì„±ëŠ¥ ì‚¬ì£¼ ê³„ì‚°ê¸°")
    print("ğŸ¯ 98ì  ì „ë¬¸ê°€ ê²€ì¦ ì™„ë£Œ")
    print("ğŸš€ LangGraph ë©€í‹° ì›Œì»¤ ì‹œìŠ¤í…œ")
    print("-" * 60)
    print("ğŸ“ ì‚¬ìš©ë²•:")
    print("  â€¢ ì‚¬ì£¼ ê³„ì‚°: '1995ë…„ 8ì›” 26ì¼ ì˜¤ì „ 10ì‹œ 15ë¶„ ë‚¨ì ì‚¬ì£¼'")
    print("  â€¢ ìš´ì„¸ ìƒë‹´: '1995ë…„ 8ì›” 26ì¼ìƒ 2024ë…„ ì—°ì• ìš´'")
    print("  â€¢ ì¼ë°˜ ê²€ìƒ‰: 'ì‚¬ì£¼ì—ì„œ ì‹­ì‹ ì´ë€?'")
    print("  â€¢ ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit'")
    print("  â€¢ ë””ë²„ê·¸: 'debug:ì§ˆë¬¸' (ìƒì„¸ ì‹¤í–‰ ì •ë³´)")
    print("  â€¢ ìƒì„¸ëª¨ë“œ: 'verbose:ì§ˆë¬¸' (ë…¸ë“œë³„ ìƒì„¸ ë¡œê¹…)")
    print("  â€¢ ê³ ê¸‰ìŠ¤íŠ¸ë¦¬ë°: 'stream:ì§ˆë¬¸' (ì£¼ìš” ë…¸ë“œë§Œ ì‹¤ì‹œê°„ ì¶œë ¥)")
    print("=" * 60)


def print_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
    print("\nğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"  â€¢ ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  â€¢ Python ë²„ì „: {sys.version.split()[0]}")
    print(f"  â€¢ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"  â€¢ ì›Œì»¤ ë…¸ë“œ: Supervisor, ì‚¬ì£¼ê³„ì‚°, RAGê²€ìƒ‰, ì›¹ê²€ìƒ‰, ì‘ë‹µìƒì„±")
    print()


def format_response(response: str) -> str:
    """ì‘ë‹µ í¬ë§·íŒ…"""
    if not response:
        return "âŒ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì‘ë‹µ ì•ì— êµ¬ë¶„ì„  ì¶”ê°€
    formatted = "\n" + "ğŸ¯ " + "=" * 55 + "\n"
    formatted += "ğŸ“‹ **FortuneAI ë¶„ì„ ê²°ê³¼**\n"
    formatted += "=" * 58 + "\n\n"
    formatted += response
    formatted += "\n\n" + "=" * 58
    
    return formatted


def print_help():
    """ë„ì›€ë§ ì¶œë ¥"""
    print("""
ğŸ“š **FortuneAI ì‚¬ìš© ê°€ì´ë“œ**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”® **ì‚¬ì£¼ ê³„ì‚°**: '1995ë…„ 8ì›” 26ì¼ ì˜¤ì „ 10ì‹œ 15ë¶„ ë‚¨ì ì‚¬ì£¼'
ğŸ“– **ì‚¬ì£¼ í•´ì„**: 'ì‚¬ì£¼ì—ì„œ ì‹­ì‹ ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?'
ğŸŒ **ì¼ë°˜ ì§ˆë¬¸**: '2024ë…„ ê°‘ì§„ë…„ì˜ íŠ¹ì§•ì€?'

ğŸ› ï¸  **íŠ¹ìˆ˜ ëª…ë ¹ì–´**:
  â€¢ new, clear      : ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘
  â€¢ help, ?         : ë„ì›€ë§ ë³´ê¸°
  â€¢ quit, exit      : í”„ë¡œê·¸ë¨ ì¢…ë£Œ
  â€¢ debug:ì§ˆë¬¸      : ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
  â€¢ verbose:ì§ˆë¬¸    : ìƒì„¸ ëª¨ë“œë¡œ ì‹¤í–‰
  â€¢ stream:ì§ˆë¬¸     : ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° (ì£¼ìš” ë…¸ë“œë§Œ)

ğŸŒŠ **ìŠ¤íŠ¸ë¦¬ë° ì„¤ëª…**:
  â€¢ ê¸°ë³¸ ì‹¤í–‰: ëª¨ë“  ë…¸ë“œì˜ í† í°ì„ ì‹¤ì‹œê°„ í‘œì‹œ (stream_graph ë°©ì‹)
  â€¢ stream: ëª…ë ¹: ì£¼ìš” ë…¸ë“œ(rag_agent, response_generator)ë§Œ í‘œì‹œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """)


def print_stream_node_header(node_name: str):
    """ìŠ¤íŠ¸ë¦¬ë° ë…¸ë“œ í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * 50)
    print(f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m ğŸ”„")
    print("- " * 25)


def print_stream_completion(is_advanced: bool = False):
    """ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥"""
    print("\n" + "=" * 50)
    if is_advanced:
        print("âœ… ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ!")
    else:
        print("âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ!")


# ================================
# ì¿¼ë¦¬ ì²˜ë¦¬ ê´€ë ¨ í•¨ìˆ˜ë“¤
# ================================

def handle_debug_query(query: str, app, conversation_history: list) -> str:
    """ë””ë²„ê·¸ ì¿¼ë¦¬ ì²˜ë¦¬"""
    if not query.startswith("debug:"):
        return None
    
    actual_query = query[6:].strip()
    if not actual_query:
        return "âŒ ë””ë²„ê·¸í•  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: debug:1995ë…„ 8ì›” 26ì¼ ì‚¬ì£¼"
    
    print(f"\nğŸ” ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘: '{actual_query}'")
    print("-" * 50)
    
    start_time = time.time()
    response = run_query_with_app(actual_query, app, conversation_history)
    execution_time = time.time() - start_time
    
    debug_info = f"""
ğŸ” **ë””ë²„ê·¸ ì •ë³´**
â€¢ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ
â€¢ ì§ˆë¬¸: {actual_query}

ğŸ“‹ **ì‘ë‹µ**
{response}
"""
    return debug_info


def handle_verbose_query(query: str, app, conversation_history: list) -> str:
    """ìƒì„¸ ëª¨ë“œ ì¿¼ë¦¬ ì²˜ë¦¬"""
    if not query.startswith("verbose:"):
        return None
    
    actual_query = query[8:].strip()
    if not actual_query:
        return "âŒ ìƒì„¸ ëª¨ë“œë¡œ ì‹¤í–‰í•  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: verbose:1995ë…„ 8ì›” 26ì¼ ì‚¬ì£¼"
    
    print(f"\nğŸ” ìƒì„¸ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘: '{actual_query}'")
    print("=" * 60)
    
    start_time = time.time()
    response = run_query_with_app(actual_query, app, conversation_history)
    execution_time = time.time() - start_time
    
    print(f"\nâ±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
    return response


def handle_stream_query(query: str, app, conversation_history: list) -> str:
    """ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬ - íŠ¹ì • ë…¸ë“œë§Œ í•„í„°ë§"""
    if not query.startswith("stream:"):
        return None
    
    actual_query = query[7:].strip()
    if not actual_query:
        return "âŒ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë°í•  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: stream:1995ë…„ 8ì›” 26ì¼ ì‚¬ì£¼"
    
    print(f"\nğŸŒŠ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘: '{actual_query}'")
    print("ğŸ“Œ ì£¼ìš” ë…¸ë“œ(rag_agent, response_generator)ë§Œ í‘œì‹œë©ë‹ˆë‹¤")
    print("=" * 60)
    
    start_time = time.time()
    response = run_query_with_streaming(actual_query, app, conversation_history)
    execution_time = time.time() - start_time
    
    print(f"\nâ±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
    return response


def run_query_with_app(query: str, app, conversation_history: list) -> str:
    """LangGraph ì‹œìŠ¤í…œìœ¼ë¡œ ì¿¼ë¦¬ ì‹¤í–‰ - stream_graph ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥"""
    print(f"ğŸ” ì¿¼ë¦¬ ì‹¤í–‰: {query}")
    
    # ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    conversation_history.append(HumanMessage(content=query))
    
    # í˜„ì¬ ìƒíƒœ ì„¤ì • (ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
    current_state = {
        "messages": conversation_history.copy(),
        "next": None,
        "final_response": None,
        "sender": None
    }
    
    try:
        print("ğŸš€ ì›Œí¬í”Œë¡œ ì‹¤í–‰ ì¤‘...")
        
        # stream_graph ë°©ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        final_response = ""
        prev_node = ""
        
        for chunk_msg, metadata in app.stream(current_state, stream_mode="messages"):
            curr_node = metadata["langgraph_node"]
            
            # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
            if curr_node != prev_node:
                print_stream_node_header(curr_node)
            
            # í† í°ë³„ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥
            if chunk_msg.content:
                print(chunk_msg.content, end="", flush=True)
            
            prev_node = curr_node
        
        print_stream_completion()
        
        # ìµœì¢… ì‘ë‹µì„ ì–»ê¸° ìœ„í•´ í•œ ë²ˆ ë” ì‹¤í–‰ (final_response íšë“ìš©)
        result = app.invoke(current_state)
        final_response = result.get("final_response")
        
        if final_response:
            # AI ì‘ë‹µë„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            conversation_history.append(AIMessage(content=final_response))
            return final_response
        else:
            print("âŒ ìµœì¢… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
            return "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def run_query_with_streaming(query: str, app, conversation_history: list) -> str:
    """í–¥ìƒëœ ìŠ¤íŠ¸ë¦¬ë° - íŠ¹ì • ë…¸ë“œë§Œ í•„í„°ë§í•˜ì—¬ ì¶œë ¥"""
    print(f"ğŸ” ì¿¼ë¦¬ ì‹¤í–‰ (ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë°): {query}")
    
    # ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    conversation_history.append(HumanMessage(content=query))
    
    current_state = {
        "messages": conversation_history.copy(),
        "next": None,
        "final_response": None,
        "sender": None
    }
    
    try:
        print("ğŸš€ ì›Œí¬í”Œë¡œ ì‹¤í–‰ ì¤‘...")
        
        # stream_graph ë°©ì‹ + ë…¸ë“œ í•„í„°ë§
        final_response = ""
        prev_node = ""
        target_nodes = ["rag_agent", "response_generator"]  # ì›í•˜ëŠ” ë…¸ë“œë§Œ ì¶œë ¥
        
        for chunk_msg, metadata in app.stream(current_state, stream_mode="messages"):
            curr_node = metadata["langgraph_node"]
            
            # íŠ¹ì • ë…¸ë“œë“¤ë§Œ ì¶œë ¥
            if curr_node in target_nodes:
                # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
                if curr_node != prev_node:
                    print_stream_node_header(curr_node)
                
                # í† í°ë³„ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥
                if chunk_msg.content:
                    print(chunk_msg.content, end="", flush=True)
                
                prev_node = curr_node
        
        print_stream_completion(is_advanced=True)
        
        # ìµœì¢… ì‘ë‹µ íšë“
        result = app.invoke(current_state)
        final_response = result.get("final_response")
        
        if final_response:
            conversation_history.append(AIMessage(content=final_response))
            return final_response
        else:
            print("âŒ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
            return run_query_with_app(query, app, conversation_history[:-1])
            
    except Exception as e:
        print(f"âŒ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("ğŸ”„ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
        return run_query_with_app(query, app, conversation_history[:-1]) 